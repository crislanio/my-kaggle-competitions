{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"abstract\">Titanic Machine Learning From Disaster</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import External Notebook: Julia Assistant Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using NBInclude\n",
    "using DecisionTree\n",
    "# nbinclude(\"../06_assistent_tools/Julia\\ Assistent\\ Tools\\ for\\ Machine\\ Learning.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = readtable(\"data/train_v2.tsv\")\n",
    "test = readtable(\"data/test_v2.tsv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Features X and Label Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = setdiff(names(test), [:Name, :PassengerId])\n",
    "feature_space = gen_feature_space(features)\n",
    "label = setdiff(names(train), names(test))[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training, Validation and Test Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val = split_train_val(train; train_size=.85, random_state=1)\n",
    "train_x, train_y, val_x, val_y = gen_train_val(train, features, label)\n",
    "dtrain, dval = gen_dtrain(train, features, label);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In External Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vw      = generate_vw_file(X_train[:, vcat(features, label)], feature_space, label)\n",
    "val_vw        = generate_vw_file(X_val[:, vcat(features, label)], feature_space, label)\n",
    "test_vw       = generate_vw_file(test[:, features], feature_space, label)\n",
    "train_vw_file = \"data/train_v2.vw\"\n",
    "val_vw_file   = \"data/val_v2.vw\"\n",
    "test_vw_file  = \"data/test_v2.vw\"\n",
    "export_gz(train_vw, train_vw_file)\n",
    "export_gz(val_vw, val_vw_file)\n",
    "export_gz(val_vw, test_vw_file);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rounds = 10\n",
    "watchlist  = [(dtrain, \"train\"), (dval, \"eval\")]\n",
    "metrics    = [\"error\"]\n",
    "params     = Dict(\"objective\"         => \"binary:logistic\",\n",
    "                   \"booster\"          => \"gbtree\",\n",
    "                   \"eta\"              => .1,\n",
    "                   \"alpha\"            => .5,\n",
    "                   \"gamma\"            => .0,\n",
    "                   \"max_depth\"        =>  5,\n",
    "                   \"colsample_bytree\" => .5,\n",
    "                   \"min_child_weight\" =>  10,\n",
    "                   \"subsample\"        => .5,\n",
    "                   \"seed\"             =>  1)\n",
    "\n",
    "println(\"Training Base Model...\")\n",
    "tic()\n",
    "model_xgb = XGBoost.xgboost(dtrain, num_rounds, param=params, metrics=metrics, watchlist=watchlist)\n",
    "toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "formulas = gen_formulas(features, feature_space, label, 10)\n",
    "model_glm = gen_glm(train, formulas, Binomial(), LogitLink());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees, Random Forests and Adaboost Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_decision_tree = train_decision_tree(train_x, train_y)\n",
    "model_random_forest = train_random_forest(train_x, train_y, random_features=5, num_trees=10, portion_samples=.5)\n",
    "model_adaboost_trees, adaboost_trees_coefs = train_adaptive_boosted_trees(train_x, train_y, num_iteration=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model ExtremyTree (1/15)...\n",
      "elapsed time: 0.023986754 seconds\n",
      "Training Model RandomForest (2/15)...\n",
      "elapsed time: 0.02914997 seconds\n",
      "Training Model PassiveAggressiveClassifier (3/15)...\n",
      "elapsed time: 0.039122004 seconds\n",
      "Training Model LogisticRegression (4/15)...\n",
      "elapsed time: 0.018073097 seconds\n",
      "Training Model Bagging_ExtremyTree (5/15)...\n",
      "elapsed time: 0.514863291 seconds\n",
      "Training Model NaiveBayes (6/15)...\n",
      "elapsed time: 0.027991561 seconds\n",
      "Training Model DecisionTree (7/15)...\n",
      "elapsed time: 0.090264383 seconds\n",
      "Training Model Bagging_DecisionTree (8/15)...\n",
      "elapsed time: 0.28740818 seconds\n",
      "Training Model kNN (9/15)...\n",
      "elapsed time: 0.063536531 seconds\n",
      "Training Model Boosting_ExtremyTree (10/15)...\n",
      "elapsed time: 0.091424457 seconds\n",
      "Training Model Boosting_DecisionTree (11/15)...\n",
      "elapsed time: 0.760145922 seconds\n",
      "Training Model SGDClassifier (12/15)...\n",
      "elapsed time: 0.045841504 seconds\n",
      "Training Model GradientBoostingTrees (13/15)...\n",
      "elapsed time: 0.361286513 seconds\n",
      "Training Model SVMClassifier (14/15)...\n",
      "elapsed time: 0.849369055 seconds\n",
      "Training Model ExtraTrees (15/15)...\n",
      "elapsed time: 0.025371992 seconds\n"
     ]
    }
   ],
   "source": [
    "models_scikit = train_scikit_models(scikit_all_classifier_models, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using l1 regularization = 0.1\n",
      "enabling BFGS based optimization **without** curvature calculation\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "m = 15\n",
      "Allocated 34M for weights and mem\n",
      "## avg. loss \tder. mag. \td. m. cond.\t wolfe1    \twolfe2    \tmix fraction\tcurvature \tdir. magnitude\tstep size \n",
      "using cache_file = data/train_v2.vw.gz.cache\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      " 1 0.69315   \t0.20299   \t2.47557   \t          \t          \t          \t27.54136  \t1264.73645\t0.08989   \n",
      "Maximum number of passes reached. If you want to optimize further, increase the number of passes\n",
      "\n",
      "finished run\n",
      "number of examples = 1364\n",
      "weighted example sum = 1364\n",
      "weighted label sum = 514\n",
      "average loss = 0.795431 h\n",
      "best constant = -0.503013\n",
      "best constant's loss = 0.662492\n",
      "total feature number = 18302\n"
     ]
    }
   ],
   "source": [
    "train_vw_binary_classifier(\"$train_vw_file.gz\", \"\", l1=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.1-pre",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
